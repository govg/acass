---
title : Machine Learning Practice and Theory
subtitle : Day 6 - Unsupervised Learning 
author : Govind Gopakumar
institute : IIT Kanpur
theme : metropolis
---

[//]: ([Lecture slides in pdf form](lec6.pdf))

# Prelude

## Announcements 

- New project groups : Meet after class for short discussion
- Programming tutorials to be put up tonight / tomorrow
- Webpage - govg.github.io/acass

## Story so far

### Supervised Learning
- KNN, Distance from means
- Decision Trees, Random Forests
- Logistic Regression, Perceptron
- Linear Regression

### Techniques
- Gradient Descent
- Formulating a loss function
- Using "maximum probability" to obtain results

# Clustering 

## Clustering - I
### Why do we need it?
- Discover patterns or "clusters"
- Preprocessing step for classification
- Allow us to learn "generative" models

### What's the easiest way to do it?
- Group objects together
- But how?

## Clustering - II

### Model overview
- K-Means clustering : defined by k points
- Each data point is assigned closest mean
- K is sort of a hyper parameter, not to be learnt!

### Training the model
- How do we find the means?
- How do we do assignment?
- What are the parameters to be learnt?

## Clustering - III

### Model parameters
- Known : Location of data
- Unknown : Cluster assignments, cluster means

### How to find both?
- Knowing cluster means let us find cluster assignments
- Knowing cluster assignments : does it help the other way around?

## Clustering - IV

### Alternating optimization
- Two different unknown parameters : $\mu, \z$
- Idea from matrix factorization.

### Finding the parameters
- How do we do alternating optimization here?
- What are the guesses?
- What does it say about our "loss function"?

## Clustering - V

### Geometry of the model
- Decision surface?
- What sort of clusters does it learn?
- When will it do badly?

### Uniqueness of clustering
- What does final cluster depend on?
- Will it always learn good clustering?
- What's an example where it will fail?
- Outliers?

# Smarter Clustering

## Gaussian Mixture Models - I
### Why should we improve our clustering?
- Hard assignment
- Logistic Regression vs other methods!
- Probabilistic interpretation

### Generative modelling
- Model how the data was generated!
- Can be used to give new data!
- Preprocessing step for supervised learning?

## Gaussian Mixture Models - III
### Review of the Gaussian distribution
- p(X) : Reflects how probable a point is
- Density decreases as distance from mean increases
- Variance reflects spread

### Estimation of a Gaussian
- Given : A bunch of data points
- What is the most likely Gaussian?
- How do we find it?


## Gaussian Mixture Models - III
### Modelling assumptions
- Assume each point is "generated" from a Gaussian
- How many Gaussians?
- Where are they?

### Model overview
- What are the unknowns in the setting?
- How do we find them?


# Dimensionality Reduction

## Aren't more features better?

### How many "features" should we have?
- Depends on problem
- Depends on our required model
- Depends on the data we collect

### How useful are these features?
- Variance?
- Entropy?
- Mean?
- "Information Gain"?

## Principal Components Analysis - I

### Model overivew
- Find "informative" directions.
- Exclude uninformative directions.
- Flexible : choose how many you want.

### What is an informative direction?
- Highest information gain?
- Do we look at combinations of features?
- How do we measure how much information we have?

## Principal Components Analysis - II

### Geometry of model
- Look at data along variance
# Conclusion

## Concluding Remarks

### Takeaways

### Announcements

## References

- [Lecture 7, CS 771 IIT Kanpur](https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec7_slides.pdf)
- [Lecture 6, CS 771 IIT Kanpur](https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec6_slides.pdf)
