<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Govind Gopakumar</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">/home/course</a>
            </div>
            <div id="navigation">
                <a href="../codes.html">Codes</a>
                <a href="../resources.html">Resources</a>
				<a href="../slides.html">Slides</a>
            </div>
        </div>

        <div id="content">
            <h1>Day 2 - Mathematical Background</h1>
            <div class="info">
    Posted on May 30, 2017
    
        by Govind Gopakumar
    
</div>

<p><a href="lec2.pdf">Lecture slides in pdf form</a></p>
<h2 id="announcements">Announcements</h2>
<ul>
<li>Pre-Course survey</li>
<li>Programming assignments</li>
<li>Project ideas and partners</li>
<li>Installation of Jupyter / IPython notebook</li>
<li>Webpage - govg.github.io/acass</li>
</ul>
<h2 id="recap">Recap</h2>
<h3 id="machine-learning">Machine Learning</h3>
<ul>
<li>Trends in data</li>
<li>Using the right model, and reasonable loss functions</li>
<li>Transforming the problem according to simplicity</li>
</ul>
<h3 id="divisions-in-machine-learning">Divisions in Machine Learning</h3>
<ul>
<li>Unsupervised learning : goal is to discover patterns in data</li>
<li>Supervised learning : goal is to predict some aspect using data</li>
</ul>
<h1 id="overview">Overview</h1>
<h2 id="notations">Notations</h2>
<h3 id="dealing-with-data">Dealing with data :</h3>
<ul>
<li>X : Data matrix (NxD)</li>
<li>Y : Label matrix (Nx1)</li>
<li>w : Model parameters</li>
<li>L(X,Y,w) : Loss of model w on X,Y</li>
</ul>
<h3 id="dealing-with-model">Dealing with model:</h3>
<ul>
<li><span class="math inline">\(\lambda\)</span> : Hyper parameters of a model</li>
<li><span class="math inline">\(w^*\)</span> : Optimal model (may or may not be unique)</li>
</ul>
<h2 id="mathematics-in-machine-learning">Mathematics in Machine Learning</h2>
<ul>
<li><p>How do we describe and manipulate data?</p>
<p>Use a matrix!</p></li>
<li><p>How do we “model” something?</p>
<p>Use a vector, or a function!</p></li>
<li><p>How do we analytically solve models?</p>
<p>Use Linear Algebra!</p></li>
<li><p>How do we mathematically “learn”?</p>
<p>Use Calculus, Linear Algebra!</p></li>
</ul>
<h1 id="probability">Probability</h1>
<h2 id="basics">Basics</h2>
<h3 id="definitions">Definitions</h3>
<ul>
<li>Event : Some occurence that is desirable</li>
<li>Sample space : All possible events</li>
<li><span class="math inline">\(P(a) = \frac{\|a\|}{\|a\| + \|a'\|}\)</span></li>
</ul>
<h3 id="terms">Terms</h3>
<ul>
<li><span class="math inline">\(\prod{p(a_i)}\)</span> - probability of multiple events</li>
<li>Can also model likelihood of event</li>
<li>Naturally leads to MLE (general technique, to be covered later)</li>
</ul>
<h2 id="random-variables">Random Variables</h2>
<h3 id="what-are-they">What are they?</h3>
<ul>
<li>Map between events and some value</li>
<li>Represented as a probability density function</li>
<li>Discrete, continuous</li>
</ul>
<h3 id="how-do-we-use-them">How do we use them?</h3>
<ul>
<li>Describe p(a) for a random variable</li>
<li>Examples include normal, beta, poisson</li>
<li>Integrate to 1</li>
</ul>
<h2 id="distributions---i">Distributions - I</h2>
<h3 id="continuous">Continuous</h3>
<ul>
<li>Gaussian : Model any real number distribution</li>
<li>Beta : Model number between [0,1]</li>
<li>Dirichlet : Model a vector that sums to 1</li>
</ul>
<h3 id="discrete">Discrete</h3>
<ul>
<li>Bernoulli : Model number of heads in a coin toss</li>
<li>Poisson : Model counts of a variable</li>
</ul>
<p>These can be combined together (joint, marginal)</p>
<h2 id="distributions---ii">Distributions - II</h2>
<h3 id="gaussian-distribution">Gaussian distribution :</h3>
<ul>
<li><span class="math inline">\(p(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-1}{2\sigma^2} (x-\mu)^2}\)</span></li>
<li><span class="math inline">\(\mu\)</span> : Mean of the distribution</li>
<li><span class="math inline">\(\sigma^2\)</span> : Variance of the distribution</li>
</ul>
<h3 id="multivariate-gaussian">Multivariate Gaussian :</h3>
<ul>
<li><span class="math inline">\(p(x) = \frac{1}{\sqrt{2\pi^k |\Sigma|}} e^{\frac{-1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}\)</span></li>
<li><span class="math inline">\(\mu\)</span> : Mean vector</li>
<li><span class="math inline">\(\Sigma\)</span> : Covariance matrix</li>
</ul>
<h2 id="distributions---iii">Distributions - III</h2>
<h3 id="multiple-variables">Multiple variables :</h3>
<ul>
<li>Define a “joint” distribution</li>
<li>Denote by p(v,u)</li>
<li>Is this the same as p(u)*p(v)? When is it not?</li>
</ul>
<h3 id="examples-in-terms-of-gaussians">Examples in terms of Gaussians :</h3>
<ul>
<li>Consider two variables, <span class="math inline">\(v \sim \mathcal{N}(\mu_v, \sigma_v)\)</span>, <span class="math inline">\(u \sim \mathcal{N}(\mu_u, \sigma_u)\)</span></li>
<li>How does the joint distribution look?</li>
<li>What if they were drawn from a 2D Gaussian?</li>
<li>When does the second case reduce to the first?</li>
</ul>
<h2 id="bayes-theorem---i">Bayes theorem - I</h2>
<h3 id="invert-the-event">Invert the event!</h3>
<ul>
<li>Reverse the probability of events</li>
<li><span class="math inline">\(P(a | b) = \frac{P(b|a)P(a)}{P(b)}\)</span></li>
</ul>
<h3 id="terms-in-this-expression">Terms in this expression</h3>
<ul>
<li><span class="math inline">\(P(a|b)\)</span> - called the posterior</li>
<li><span class="math inline">\(P(b|a)\)</span> - called the likelihood</li>
<li><span class="math inline">\(P(a)\)</span> - called the prior</li>
</ul>
<h2 id="bayes-theorem---ii">Bayes theorem - II</h2>
<h3 id="setting">Setting</h3>
<ul>
<li>B : Color of the ball</li>
<li>A : Selection of box</li>
<li><span class="math inline">\(B_1(1, 1, 1), B_2(2, 0, 0), B_3(0, 0, 1)\)</span></li>
<li>All boxes are equally likely</li>
</ul>
<h3 id="inverting-the-event">Inverting the event</h3>
<ul>
<li>P(b | a) : Probability that color was b given box is a.</li>
<li>P(a | b) : Probability that box was a given color is b.</li>
<li>How do we use Bayes theorem here?</li>
</ul>
<h1 id="statistics">Statistics</h1>
<h2 id="statistics-of-a-sample---i">Statistics of a sample - I</h2>
<h3 id="mean-of-sample">Mean of sample</h3>
<ul>
<li><span class="math inline">\(\mathbb{E}[X]\)</span> - “average” of the distribution</li>
<li>When can it be useless?</li>
<li>When can it work as a representation?</li>
</ul>
<h3 id="variances-and-covariances">Variances and covariances</h3>
<ul>
<li><span class="math inline">\(\sigma^2\)</span> - “spread” of the distribution</li>
<li>Can be used to “normalize” data</li>
<li>Can be used to see where data is useless</li>
</ul>
<p>Generally, we do not come across other “moments” of the data in Machine Learning (skew, kurtosis etc).</p>
<h2 id="statistics-of-a-sample---ii">Statistics of a sample - II</h2>
<h3 id="of-standard-distributions">Of standard distributions</h3>
<ul>
<li>Gaussian : <span class="math inline">\(\Sigma\)</span></li>
<li>Bernoulli : <span class="math inline">\(p(1-p)\)</span></li>
</ul>
<h3 id="of-a-sample">Of a sample</h3>
<ul>
<li>Defined as “empirical” quantities</li>
<li>Mean : <span class="math inline">\(\mu\)</span></li>
<li>Variance / Covariance</li>
<li>Used in “moment matching” techniques</li>
</ul>
<h1 id="linear-algebra">Linear Algebra</h1>
<h2 id="spaces">Spaces</h2>
<h3 id="constituents">Constituents :</h3>
<ul>
<li>Vectors (v,u,w)</li>
<li>Dot products</li>
<li>Norms</li>
</ul>
<h3 id="utility">Utility :</h3>
<ul>
<li>Our data “lives” in some space</li>
<li>Our model describes “shapes” in that space</li>
<li>Must deal with math of this space!</li>
</ul>
<h2 id="matrix-algebra">Matrix Algebra</h2>
<h3 id="basics-1">Basics</h3>
<ul>
<li>Matrix (NxD) : Can denote a set of points</li>
<li>Vector (1xD) : Denotes a single point</li>
<li>Usually denotes our data</li>
</ul>
<h3 id="properties">Properties</h3>
<ul>
<li>Invertibility : <span class="math inline">\(AA^{-1} = I\)</span></li>
<li>Definiteness : PD / PSD</li>
</ul>
<h2 id="other-terms">Other terms</h2>
<h3 id="eigenvalues">Eigenvalues</h3>
<ul>
<li><span class="math inline">\(Av = \lambda v\)</span> : <span class="math inline">\(\lambda\)</span> is an “eigenvalue”</li>
<li>Denotes a direction in the space of the matrix</li>
</ul>
<h3 id="measures-of-vectors">Measures of vectors</h3>
<ul>
<li><span class="math inline">\(\|x\|_p\)</span> - denotes the p-norm</li>
<li>Different norms have different interpretations</li>
<li>Similarities (cos, distance)</li>
</ul>
<h1 id="functions-and-optimization">Functions and Optimization</h1>
<h2 id="function-shapes">Function shapes</h2>
<h3 id="convexity">Convexity</h3>
<ul>
<li>Convex (and concave) functions have single optima</li>
<li>Easy to optimize over</li>
<li>Follow the slope method</li>
<li>Closed under summation (this is very very nice and important!)</li>
</ul>
<h3 id="smoothness-and-differentiability">Smoothness and differentiability</h3>
<ul>
<li>If a function is “smooth”, it will be easy to find the slope.</li>
<li>If it has kinks, slightly harder to find actual gradients!</li>
<li>If it is discontinuous, no real way to find gradients!</li>
</ul>
<h2 id="optimization-theory">Optimization theory</h2>
<h3 id="basics-2">Basics :</h3>
<ul>
<li>Gradient descent : how to follow the slope</li>
<li>Simple gradients for simple loss functions</li>
<li>Combine gradients for sum of functions</li>
</ul>
<h3 id="examples-of-gradients">Examples of gradients :</h3>
<ul>
<li><span class="math inline">\((w-x)^2\)</span> : <span class="math inline">\(2(w-x)\)</span></li>
<li><span class="math inline">\(e^{-w}\)</span> : <span class="math inline">\(-e^{-w}\)</span></li>
</ul>
<h2 id="example-of-gradient-descent">Example of gradient descent</h2>
<ul>
<li>For simple functions, easy to compute gradients</li>
<li>General form of GD : <span class="math inline">\(x^{t+1} = x^t - \eta g^t\)</span></li>
<li>Consider : <span class="math inline">\(f(x) = (x+c)^2\)</span></li>
<li>Gradient : <span class="math inline">\(g(x) = 2(x+c)\)</span></li>
</ul>
<p>Let’s do gradient descent on this!</p>
<h1 id="modelling">Modelling</h1>
<h2 id="probabilistic-modelling">Probabilistic modelling</h2>
<h3 id="coin-tossing-model">Coin tossing : model</h3>
<ul>
<li>What do we wish to model? : bias of coin (k)</li>
<li>What data do we have? : H heads, T tails observed</li>
</ul>
<h3 id="mle-modelling">MLE modelling</h3>
<ul>
<li>p(H heads, T tails)?</li>
<li>What can we do with this now?</li>
<li>“Likelihood” can be our loss!</li>
<li>What is the optimal choice here?</li>
<li>Why could this fail?</li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>How to write down probability of events</li>
<li>What the mean and variance tell us about a random quantity</li>
<li>Why matrices are used in Machine Learning, how we manipulate them</li>
<li>What sort of loss functions should we consider? How do we actually use them?</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="http://web.cse.iitk.ac.in/users/piyush/tmp/maths_refresher.pdf">Review lecture in CS771, IIT Kanpur</a></li>
<li><a href="http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf">Linear Algebra Overview</a></li>
<li><a href="http://cs229.stanford.edu/section/cs229-prob.pdf">Probability Overview</a></li>
<li><a href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf">Matrix Algebra Overview</a></li>
</ul>
<h1 id="next-lecture-overview">Next Lecture overview</h1>
<h2 id="our-first-classifier">Our first classifier</h2>
<h3 id="naive-method-of-doing-classification">Naive method of doing classification?</h3>
<ul>
<li>Choose points which are nearby?</li>
<li>Choose cluster which is nearby?</li>
</ul>
<h3 id="formal-names">Formal “names”</h3>
<ul>
<li>K-nearest Neighbors</li>
<li>Distance from means</li>
</ul>
<h2 id="distance-from-means---i">Distance from means - I</h2>
<h3 id="overview-of-model">Overview of model</h3>
<ul>
<li>Compute center of each class / label</li>
<li>Assign the new point to closest mean</li>
<li>What does “training” mean now?</li>
<li>What does “testing” mean now?</li>
</ul>
<h3 id="drawbacks-and-strengths">Drawbacks and strengths?</h3>
<ul>
<li>Storage?</li>
<li>Time taken?</li>
<li>When can this be a bad method?</li>
<li>When can this be good?</li>
</ul>
<h2 id="distance-from-means---ii">Distance from means - II</h2>
<h3 id="coming-up-with-our-decision-function">Coming up with our “decision function”</h3>
<ul>
<li><span class="math inline">\(\mu_+\)</span> : positive mean</li>
<li><span class="math inline">\(\mu_-\)</span> : negative mean</li>
<li><span class="math inline">\(f(x^{new}) = d(x^{new}, \mu_-) - d(x^{new}, \mu_+)\)</span></li>
</ul>
<h3 id="geometry-of-the-decision-function">Geometry of the decision function</h3>
<ul>
<li>What does the boundary look like for this?</li>
<li>What can it learn? What can’t it learn?</li>
</ul>
<h2 id="distance-from-means---iii">Distance from means - III</h2>
<h3 id="as-similarity-to-training-data">As similarity to training data</h3>
<ul>
<li><span class="math inline">\(\|x^{new} - \mu_- \|^2 - \|x^{new} - \mu_+ \|^2\)</span></li>
<li><span class="math inline">\(\langle \mu_+ - \mu_-, x^{new} \rangle + C\)</span></li>
<li>Can be simplified into : <span class="math inline">\(f(x^{new}) = \sum \alpha_i \langle x_i, x^{new} \rangle + B\)</span></li>
</ul>
<p>What does this mean?</p>
<h2 id="knn---i">KNN - I</h2>
<h3 id="overview-of-model-1">Overview of model</h3>
<ul>
<li>Assign each point the class / value of its neighbor</li>
<li>“K” - how many neighbors you account for</li>
<li>What does “training” mean here?</li>
<li>What would “testing” mean?</li>
</ul>
<h3 id="drawbacks-and-strenghts">Drawbacks and strenghts?</h3>
<ul>
<li>Storage?</li>
<li>Time taken</li>
<li>When can this be good or bad?</li>
</ul>
<h2 id="knn---ii">KNN - II</h2>
<h3 id="geometry-of-the-decision-function-1">Geometry of the decision function</h3>
<ul>
<li>What sort of boundary does this generate?</li>
<li>How powerful can this be?</li>
<li>The “distance” can always be measured in other forms!</li>
</ul>
<h3 id="things-to-consider-for-this-model">Things to consider for this model</h3>
<ul>
<li>What happens if we have outliers?</li>
<li>Where could this be an issue?</li>
</ul>
<h2 id="knn---iii">KNN - III</h2>
<h3 id="what-is-the-optimal-k">What is the optimal K?</h3>
<ul>
<li>What happens if we increase K?</li>
<li>Consider limit of K -&gt; N?</li>
<li>What’s the best choice then?</li>
</ul>
<h3 id="extensions-to-knn">Extensions to KNN</h3>
<ul>
<li>Can this be extended in the regression / labelling setting?</li>
<li>Transformation of coordinates - How does that affect KNN?</li>
</ul>


        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
