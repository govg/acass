<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Govind Gopakumar</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">/home</a>
            </div>
            <div id="navigation">
                <a href="../codes.html">Codes</a>
                <a href="../resources.html">Resources</a>
				<a href="../slides.html">Slides</a>
            </div>
        </div>

        <div id="content">
            <h1>Day 6 - Unsupervised Learning</h1>
            <div class="info">
    Posted on June  5, 2017
    
        by Govind Gopakumar
    
</div>

<p><a href="lec6.pdf">Lecture slides in pdf form</a></p>
<h1 id="prelude">Prelude</h1>
<h2 id="announcements">Announcements</h2>
<ul>
<li>Project Groups : Code should be up and running, you should have some idea of what your project is and what the end goals are.</li>
<li>Quiz 1 is up : Auto graded, feedback. Ask if you have doubts</li>
<li>Programming assignment 1 is up : Gradient descent</li>
</ul>
<h2 id="story-so-far">Story so far</h2>
<h3 id="supervised-learning">Supervised Learning</h3>
<ul>
<li>KNN, Distance from means</li>
<li>Decision Trees, Random Forests</li>
<li>Logistic Regression, Perceptron</li>
<li>Linear Regression</li>
</ul>
<h3 id="techniques">Techniques</h3>
<ul>
<li>Gradient Descent</li>
<li>Formulating a loss function</li>
<li>Using “maximum probability” to obtain results</li>
</ul>
<h1 id="clustering">Clustering</h1>
<h2 id="clustering---i">Clustering - I</h2>
<h3 id="why-do-we-need-it">Why do we need it?</h3>
<ul>
<li>Discover patterns or “clusters”</li>
<li>Preprocessing step for classification</li>
<li>Allow us to learn “generative” models</li>
</ul>
<h3 id="whats-the-easiest-way-to-do-it">What’s the easiest way to do it?</h3>
<ul>
<li>Group objects together</li>
<li>But how?</li>
</ul>
<h2 id="clustering---ii">Clustering - II</h2>
<h3 id="model-overview">Model overview</h3>
<ul>
<li>K-Means clustering : defined by k points</li>
<li>Each data point is assigned closest mean</li>
<li>K is sort of a hyper parameter, not to be learnt!</li>
</ul>
<h3 id="training-the-model">Training the model</h3>
<ul>
<li>How do we find the means?</li>
<li>How do we do assignment?</li>
<li>What are the parameters to be learnt?</li>
</ul>
<h2 id="clustering---iii">Clustering - III</h2>
<h3 id="model-parameters">Model parameters</h3>
<ul>
<li>Known : Location of data</li>
<li>Unknown : Cluster assignments, cluster means</li>
</ul>
<h3 id="how-to-find-both">How to find both?</h3>
<ul>
<li>Knowing cluster means let us find cluster assignments</li>
<li>Knowing cluster assignments : does it help the other way around?</li>
</ul>
<h2 id="clustering---iv">Clustering - IV</h2>
<h3 id="alternating-optimization">Alternating optimization</h3>
<ul>
<li>Two different unknown parameters : <span class="math inline">\(\mu, z\)</span></li>
<li>Idea from matrix factorization.</li>
</ul>
<h3 id="finding-the-parameters">Finding the parameters</h3>
<ul>
<li>How do we do alternating optimization here?</li>
<li>What are the guesses?</li>
<li>What does it say about our “loss function”?</li>
</ul>
<h2 id="clustering---v">Clustering - V</h2>
<h3 id="estimating-the-cluster-ids">Estimating the cluster IDs</h3>
<ul>
<li>Iterate over all the means</li>
<li>Assign cluster ID as the closest mean</li>
</ul>
<h3 id="estimating-the-cluster-means">Estimating the cluster means</h3>
<ul>
<li>Collect points belonging to specific cluster</li>
<li>Compute mean of that cluster</li>
</ul>
<h2 id="clustering---vi">Clustering - VI</h2>
<h3 id="geometry-of-the-model">Geometry of the model</h3>
<ul>
<li>Decision surface?</li>
<li>What sort of clusters does it learn?</li>
<li>When will it do badly?</li>
</ul>
<h3 id="uniqueness-of-clustering">Uniqueness of clustering</h3>
<ul>
<li>What does final cluster depend on?</li>
<li>Will it always learn good clustering?</li>
<li>What’s an example where it will fail?</li>
<li>Outliers?</li>
</ul>
<h2 id="clustering---vii">Clustering - VII</h2>
<h3 id="comments-about-k-means">Comments about K-Means</h3>
<ul>
<li>Makes hard assignments</li>
<li>Size of clusters matters!</li>
<li>Can work with transformations as well!</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li>Non-convexity of the “loss” function!</li>
<li>Iterative solution</li>
<li>Will have to work with better notions of “distance”!</li>
<li>How do we choose k?</li>
</ul>
<h1 id="smarter-clustering">Smarter Clustering</h1>
<h2 id="gaussian-mixture-models---i">Gaussian Mixture Models - I</h2>
<h3 id="why-should-we-improve-our-clustering">Why should we improve our clustering?</h3>
<ul>
<li>Hard assignment</li>
<li>Logistic Regression vs other methods!</li>
<li>Probabilistic interpretation</li>
</ul>
<h3 id="generative-modelling">Generative modelling</h3>
<ul>
<li>Model how the data was generated!</li>
<li>Can be used to give new data!</li>
<li>Preprocessing step for supervised learning?</li>
</ul>
<h2 id="gaussian-mixture-models---ii">Gaussian Mixture Models - II</h2>
<h3 id="review-of-the-gaussian-distribution">Review of the Gaussian distribution</h3>
<ul>
<li>p(X) : Reflects how probable a point is</li>
<li>Density decreases as distance from mean increases</li>
<li>Variance reflects spread</li>
</ul>
<h3 id="estimation-of-a-gaussian">Estimation of a Gaussian</h3>
<ul>
<li>Given : A bunch of data points</li>
<li>What is the most likely Gaussian?</li>
<li>How do we find it?</li>
</ul>
<h2 id="gaussian-mixture-models---iii">Gaussian Mixture Models - III</h2>
<h3 id="modelling-assumptions">Modelling assumptions</h3>
<ul>
<li>Assume each point is “generated” from a Gaussian</li>
<li>How many Gaussians?</li>
<li>Where are they?</li>
</ul>
<h3 id="model-overview-1">Model overview</h3>
<ul>
<li>What are the unknowns in the setting?</li>
<li>How do we find them?</li>
</ul>
<h2 id="gaussian-mixture-models---iv">Gaussian Mixture Models - IV</h2>
<h3 id="alternating-optimization-1">Alternating optimization?</h3>
<ul>
<li>Find cluster ID’s in a probabilistic sense</li>
<li>Find clusters also in the same fashion!</li>
</ul>
<h3 id="what-are-the-parameters-then">What are the parameters then?</h3>
<ul>
<li><span class="math inline">\(\mu\)</span> for each Gaussian</li>
<li><span class="math inline">\(\Sigma\)</span> for each Gaussian</li>
<li>Can we make intelligent choices here?</li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<h2 id="concluding-remarks">Concluding Remarks</h2>
<h3 id="takeaways">Takeaways</h3>
<ul>
<li>How to cluster points for unsupervised learning</li>
<li>How to do alternating optimization (2nd such example)</li>
<li>Generative modelling and Gaussian Mixture models</li>
</ul>
<h3 id="announcements-1">Announcements</h3>
<ul>
<li>Assignment 1, Quiz 1 up.</li>
<li>(Hopefully programming tutorials also up)</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec10_slides.pdf">Lecture 10, CS 771 IIT Kanpur</a></li>
<li><a href="https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec16_slides.pdf">Lecture 16, CS 771 IIT Kanpur</a></li>
</ul>


        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
