<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Govind Gopakumar</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">/home</a>
            </div>
            <div id="navigation">
                <a href="../codes.html">Codes</a>
                <a href="../resources.html">Resources</a>
				<a href="../slides.html">Slides</a>
            </div>
        </div>

        <div id="content">
            <h1>Machine Learning Practice and Theory</h1>
            <div class="info">
    Posted on June  5, 2017
    
        by Govind Gopakumar
    
</div>

<h1 id="prelude">Prelude</h1>
<h2 id="announcements">Announcements</h2>
<ul>
<li>New project groups : Meet after class for short discussion</li>
<li>Programming tutorials to be put up tonight / tomorrow</li>
<li>Webpage - govg.github.io/acass</li>
</ul>
<h2 id="story-so-far">Story so far</h2>
<h3 id="supervised-learning">Supervised Learning</h3>
<ul>
<li>KNN, Distance from means</li>
<li>Decision Trees, Random Forests</li>
<li>Logistic Regression, Perceptron</li>
<li>Linear Regression</li>
</ul>
<h3 id="techniques">Techniques</h3>
<ul>
<li>Gradient Descent</li>
<li>Formulating a loss function</li>
<li>Using “maximum probability” to obtain results</li>
</ul>
<h1 id="clustering">Clustering</h1>
<h2 id="clustering---i">Clustering - I</h2>
<h3 id="why-do-we-need-it">Why do we need it?</h3>
<ul>
<li>Discover patterns or “clusters”</li>
<li>Preprocessing step for classification</li>
<li>Allow us to learn “generative” models</li>
</ul>
<h3 id="whats-the-easiest-way-to-do-it">What’s the easiest way to do it?</h3>
<ul>
<li>Group objects together</li>
<li>But how?</li>
</ul>
<h2 id="clustering---ii">Clustering - II</h2>
<h3 id="model-overview">Model overview</h3>
<ul>
<li>K-Means clustering : defined by k points</li>
<li>Each data point is assigned closest mean</li>
<li>K is sort of a hyper parameter, not to be learnt!</li>
</ul>
<h3 id="training-the-model">Training the model</h3>
<ul>
<li>How do we find the means?</li>
<li>How do we do assignment?</li>
<li>What are the parameters to be learnt?</li>
</ul>
<h2 id="clustering---iii">Clustering - III</h2>
<h3 id="model-parameters">Model parameters</h3>
<ul>
<li>Known : Location of data</li>
<li>Unknown : Cluster assignments, cluster means</li>
</ul>
<h3 id="how-to-find-both">How to find both?</h3>
<ul>
<li>Knowing cluster means let us find cluster assignments</li>
<li>Knowing cluster assignments : does it help the other way around?</li>
</ul>
<h2 id="clustering---iv">Clustering - IV</h2>
<h3 id="alternating-optimization">Alternating optimization</h3>
<ul>
<li>Two different unknown parameters : <span class="math inline">\(\mu, \z\)</span></li>
<li>Idea from matrix factorization.</li>
</ul>
<h3 id="finding-the-parameters">Finding the parameters</h3>
<ul>
<li>How do we do alternating optimization here?</li>
<li>What are the guesses?</li>
<li>What does it say about our “loss function”?</li>
</ul>
<h2 id="clustering---v">Clustering - V</h2>
<h3 id="geometry-of-the-model">Geometry of the model</h3>
<ul>
<li>Decision surface?</li>
<li>What sort of clusters does it learn?</li>
<li>When will it do badly?</li>
</ul>
<h3 id="uniqueness-of-clustering">Uniqueness of clustering</h3>
<ul>
<li>What does final cluster depend on?</li>
<li>Will it always learn good clustering?</li>
<li>What’s an example where it will fail?</li>
<li>Outliers?</li>
</ul>
<h1 id="smarter-clustering">Smarter Clustering</h1>
<h2 id="gaussian-mixture-models---i">Gaussian Mixture Models - I</h2>
<h3 id="why-should-we-improve-our-clustering">Why should we improve our clustering?</h3>
<ul>
<li>Hard assignment</li>
<li>Logistic Regression vs other methods!</li>
<li>Probabilistic interpretation</li>
</ul>
<h3 id="generative-modelling">Generative modelling</h3>
<ul>
<li>Model how the data was generated!</li>
<li>Can be used to give new data!</li>
<li>Preprocessing step for supervised learning?</li>
</ul>
<h2 id="gaussian-mixture-models---iii">Gaussian Mixture Models - III</h2>
<h3 id="review-of-the-gaussian-distribution">Review of the Gaussian distribution</h3>
<ul>
<li>p(X) : Reflects how probable a point is</li>
<li>Density decreases as distance from mean increases</li>
<li>Variance reflects spread</li>
</ul>
<h3 id="estimation-of-a-gaussian">Estimation of a Gaussian</h3>
<ul>
<li>Given : A bunch of data points</li>
<li>What is the most likely Gaussian?</li>
<li>How do we find it?</li>
</ul>
<h2 id="gaussian-mixture-models---iii-1">Gaussian Mixture Models - III</h2>
<h3 id="modelling-assumptions">Modelling assumptions</h3>
<ul>
<li>Assume each point is “generated” from a Gaussian</li>
<li>How many Gaussians?</li>
<li>Where are they?</li>
</ul>
<h3 id="model-overview-1">Model overview</h3>
<ul>
<li>What are the unknowns in the setting?</li>
<li>How do we find them?</li>
</ul>
<h1 id="dimensionality-reduction">Dimensionality Reduction</h1>
<h2 id="arent-more-features-better">Aren’t more features better?</h2>
<h3 id="how-many-features-should-we-have">How many “features” should we have?</h3>
<ul>
<li>Depends on problem</li>
<li>Depends on our required model</li>
<li>Depends on the data we collect</li>
</ul>
<h3 id="how-useful-are-these-features">How useful are these features?</h3>
<ul>
<li>Variance?</li>
<li>Entropy?</li>
<li>Mean?</li>
<li>“Information Gain”?</li>
</ul>
<h2 id="principal-components-analysis---i">Principal Components Analysis - I</h2>
<h3 id="model-overivew">Model overivew</h3>
<ul>
<li>Find “informative” directions.</li>
<li>Exclude uninformative directions.</li>
<li>Flexible : choose how many you want.</li>
</ul>
<h3 id="what-is-an-informative-direction">What is an informative direction?</h3>
<ul>
<li>Highest information gain?</li>
<li>Do we look at combinations of features?</li>
<li>How do we measure how much information we have?</li>
</ul>
<h2 id="principal-components-analysis---ii">Principal Components Analysis - II</h2>
<h3 id="geometry-of-model">Geometry of model</h3>
<ul>
<li>Look at data along variance # Conclusion</li>
</ul>
<h2 id="concluding-remarks">Concluding Remarks</h2>
<h3 id="takeaways">Takeaways</h3>
<h3 id="announcements-1">Announcements</h3>
<h2 id="references">References</h2>
<ul>
<li><a href="https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec7_slides.pdf">Lecture 7, CS 771 IIT Kanpur</a></li>
<li><a href="https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec6_slides.pdf">Lecture 6, CS 771 IIT Kanpur</a></li>
</ul>


        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
