<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Govind Gopakumar</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">/home</a>
            </div>
            <div id="navigation">
                <a href="../codes.html">Codes</a>
                <a href="../resources.html">Resources</a>
				<a href="../slides.html">Slides</a>
            </div>
        </div>

        <div id="content">
            <h1>Tutorial - Principal Components Analysis</h1>
            <div class="info">
    Posted on June  8, 2017
    
        by Govind Gopakumar
    
</div>

<p>Please find the associated IPython file <a href="https://github.com/govg/acass/blob/master/code/PCA.ipynb">here</a></p>
<h2 id="principal-components-analysis">Principal Components Analysis</h2>
<p>In this notebook, we shall see how we can use PCA to reduce dimensionality. Also, with the included plots, hopefully an intuition as to how it works can be built.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Import the required libraries</span>
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> sklearn <span class="im">as</span> sk
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># We'll create some data and see what PCA discovers in them. For this, we will draw samples from a Gaussian distribution</span>
<span class="co"># Our data is centered about origin</span>
mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]
<span class="co"># Our data will have covariance as so. See that the last two features are correlated!</span>
cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>], [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>]]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># We will generate 500 points lying in 4D space, generated by the gaussian distribution with given mean and covariance</span>
X <span class="op">=</span> np.random.multivariate_normal(mean, cov, <span class="dv">500</span>)
<span class="bu">print</span>(X.shape)</code></pre></div>
<pre><code>(500, 4)</code></pre>
<p>Notice that our data is composed of 4 “features” now. The last two have higher variance (4), and are also correlated. That means if we do PCA, we should “discover” 1 direction along which the spread is maximum, then two directions which have reduced spread (first two features are unrelated, and don’t spread), and the fourth direction will not have any spread (since features are perfectly correlated!). Feel free to play with different settings of the Gaussian distribution!</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA
pca <span class="op">=</span> PCA()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pca.fit(X)</code></pre></div>
<pre><code>PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Now, we will print how much &quot;variance&quot; each of the principal components explains. Roughly speaking, it tells us how much % of the spread lies</span>
<span class="co"># along each direction</span>
plt.plot(pca.explained_variance_ratio_)
plt.show()</code></pre></div>
<div class="figure">
<img src="../images/code7_1.png" alt="Generated Image" />
<p class="caption">Generated Image</p>
</div>
<p>As we can see above, the first component explains 80% of the total data spread. The second and third component explains roughly 10% each, and the last component explains nothing!</p>
<h3 id="pca-on-real-dataset">PCA on real dataset</h3>
<p>The MNIST <a href="http://yann.lecun.com/exdb/mnist/">dataset</a> is a famous set of photographs, each showing one handwritten digit. We will work with a processed version of the data set. You can find the dataset in my github <a href="https://github.com/govg/acass/tree/master/data">repository</a>. Please download the different datasets. The folder for each data set will also contain python files that show how to convert from raw data -&gt; python array. Please take a look at those.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Now let's work with some real data!</span>
<span class="im">import</span> os, sys</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># The MNIST dataset defines images of the size 28x28. We treat the entire image as a row!</span>
X <span class="op">=</span> np.load(<span class="st">&quot;../data/mnist/Xtrain.npy&quot;</span>)
<span class="bu">print</span>(X.shape)</code></pre></div>
<pre><code>(11022, 784)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Since we assume images to be very correlated, we can now do PCA and see the effect of it on reducing dimensions!</span>
<span class="co"># Note that this may time some time depending on how powerful your machine is. On my machine, doing PCA on the entire</span>
<span class="co"># MNIST dataset that was loaded took a few seconds.</span>
pca2 <span class="op">=</span> PCA()
pca2.fit(X)</code></pre></div>
<pre><code>PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.plot(pca2.explained_variance_ratio_)
plt.show()</code></pre></div>
<div class="figure">
<img src="../images/code7_2.png" alt="Generated Image" />
<p class="caption">Generated Image</p>
</div>
<p>This is amazing! It tells us that of 784 possible diretions, only the first 100 or so are really important. Let’s try out a very toy classification problem then, to see if this is actually the case.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Y <span class="op">=</span> np.load(<span class="st">&quot;../data/mnist/Ytrain.npy&quot;</span>)
<span class="bu">print</span>(Y.shape)</code></pre></div>
<pre><code>(11022,)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Xtr <span class="op">=</span> pca2.transform(X)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Choose only the first 100 features</span>
Xtr <span class="op">=</span> Xtr[:,<span class="dv">0</span>:<span class="dv">100</span>]
<span class="bu">print</span>(Xtr.shape)</code></pre></div>
<pre><code>(11022, 99)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Now let's load the test data!</span>
Xts <span class="op">=</span> np.load(<span class="st">&quot;../data/mnist/Xtest.npy&quot;</span>)
Yts <span class="op">=</span> np.load(<span class="st">&quot;../data/mnist/Ytest.npy&quot;</span>)
<span class="bu">print</span>(Xts.shape)
<span class="bu">print</span>(Yts.shape)</code></pre></div>
<pre><code>(2010, 784)
(2010,)</code></pre>
<p>We will now compare the effect of PCA, by training a classifier on both our original data, as well as the transformed data. If the latter gives us as good performance as the former, then doing PCA was actually useful! Let’s use a Random Forest classifier here, of course, you can use whatever you feel like.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier

<span class="co"># The parameters are : 20 trees, each tree going up to 20 depth, and allow training of 3 trees in parallel. If you have a less powerful machine, you might want to reduce</span>
<span class="co"># the n_jobs parameter to 1 or 2. </span>
clf1 <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">30</span>, max_depth <span class="op">=</span> <span class="dv">10</span>, n_jobs <span class="op">=</span> <span class="dv">3</span>)
clf2 <span class="op">=</span> RandomForestClassifier(n_estimators <span class="op">=</span> <span class="dv">30</span>, max_depth <span class="op">=</span> <span class="dv">10</span>, n_jobs <span class="op">=</span> <span class="dv">3</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Train the first classifier on the original data</span>
clf1.fit(X, Y)</code></pre></div>
<pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=10, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=30, n_jobs=3, oob_score=False, random_state=None,
            verbose=0, warm_start=False)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Train the second classifier on the transformed data, only the first 100 directions</span>
clf2.fit(Xtr, Y)</code></pre></div>
<pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=10, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=30, n_jobs=3, oob_score=False, random_state=None,
            verbose=0, warm_start=False)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Find the accuracy of the first classifier on the test data</span>
<span class="bu">print</span>((Yts <span class="op">==</span> clf1.predict(Xts)).mean())</code></pre></div>
<pre><code>0.993532338308</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Find the accuracy of the second classifier on the test data. Notice that we need to transform the testing data according to our PCA model before feeding it</span>
<span class="co"># into the classifier</span>
<span class="bu">print</span>((Yts <span class="op">==</span> clf2.predict(pca2.transform(Xts))).mean())</code></pre></div>
<pre><code>0.946766169154</code></pre>
<p>Alas! Doing PCA did not help in this situation. It could be because of multiple reasons. One simple reason could be that our Random Forest classifier does not try to exploit the geometry of the problem by finding seperating lines. In this case, if the features were correlated and had some sort of linear seperation, doing PCA actually destroys that geometry somewhat. This should tell us that using PCA blindly can also be an issue.</p>


        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
