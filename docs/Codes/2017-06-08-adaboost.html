<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Govind Gopakumar</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">/home</a>
            </div>
            <div id="navigation">
                <a href="../codes.html">Codes</a>
                <a href="../resources.html">Resources</a>
				<a href="../slides.html">Slides</a>
            </div>
        </div>

        <div id="content">
            <h1>Tutorial - AdaBoost decision Surfaces</h1>
            <div class="info">
    Posted on June  8, 2017
    
        by Govind Gopakumar
    
</div>

<p>Please find the associated IPython file <a href="https://github.com/govg/acass/blob/master/code/Adaboost.ipynb">here</a></p>
<h1 id="adaboost-tutorial">AdaBoost tutorial</h1>
<p>This is adapted from scikit learn docs <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py">here</a>. All rights and credits belong to them! This will take you through what the decision surfaces look like when we use the general technique of boosting to learn a powerful classifier from a set of weak classifiers.</p>
<p>We will fit our data onto a group of Gaussians. We shall plot and see how it is not easy for a single weak classifier to predict properly the class disribution, but boosting enables us to learn the entire structure.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Author: Noel Dawe &lt;noel.dawe@gmail.com&gt;</span>
<span class="co"># Modified by : Govind Gopakumar &lt;govindg@cse.iitk.ac.in&gt;</span>
<span class="co"># License: BSD 3 clause</span>

<span class="co"># Let us import the basic libraries</span>
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt

<span class="co"># We shall import the different classifier libraries we will need!</span>
<span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier
<span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier
<span class="im">from</span> sklearn.datasets <span class="im">import</span> make_gaussian_quantiles</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Construct dataset</span>
X1, y1 <span class="op">=</span> make_gaussian_quantiles(cov<span class="op">=</span><span class="dv">2</span>.,
                                 n_samples<span class="op">=</span><span class="dv">200</span>, n_features<span class="op">=</span><span class="dv">2</span>,
                                 n_classes<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">1</span>)
X2, y2 <span class="op">=</span> make_gaussian_quantiles(mean<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), cov<span class="op">=</span><span class="fl">1.5</span>,
                                 n_samples<span class="op">=</span><span class="dv">300</span>, n_features<span class="op">=</span><span class="dv">2</span>,
                                 n_classes<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">1</span>)

<span class="co"># Concatenate the two constructed bits together</span>
X <span class="op">=</span> np.concatenate((X1, X2))
y <span class="op">=</span> np.concatenate((y1, <span class="op">-</span> y2 <span class="op">+</span> <span class="dv">1</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Let's print the size of the training data set</span>
<span class="bu">print</span>(X.shape)
<span class="bu">print</span>(y.shape)</code></pre></div>
<pre><code>(500, 2)
(500,)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># This plot should show us how our data can't be cleanly seperated</span>
plt.scatter(X[:,<span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y)
plt.show()</code></pre></div>
<div class="figure">
<img src="../images/code8_1.png" alt="Generated Image" />
<p class="caption">Generated Image</p>
</div>
<p>In AdaBoost, we choose a “base classifier” to start boosting. We shall work with a Decision Tree classifier, but note that we can choose whatever we want.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create a small Adaboosted decision tree, this time with 2 trees</span>
bdt_small <span class="op">=</span> AdaBoostClassifier(DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),
                         algorithm<span class="op">=</span><span class="st">&quot;SAMME&quot;</span>,
                         n_estimators<span class="op">=</span><span class="dv">2</span>)

<span class="co"># Create a larger Adaboosted decision tree, this time with 200 trees!</span>
bdt_big <span class="op">=</span> AdaBoostClassifier(DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),
                         algorithm<span class="op">=</span><span class="st">&quot;SAMME&quot;</span>,
                         n_estimators<span class="op">=</span><span class="dv">200</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Fit both the trees</span>
bdt_small.fit(X, y)
bdt_big.fit(X, y)</code></pre></div>
<pre><code>AdaBoostClassifier(algorithm='SAMME',
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best'),
          learning_rate=1.0, n_estimators=200, random_state=None)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Set this to bdt_small or bdt_big to see the different decision boundaries</span>
bdt <span class="op">=</span> bdt_big


<span class="co"># The rest of this is plotting code. You don't need to mess with this really</span>
plot_colors <span class="op">=</span> <span class="st">&quot;br&quot;</span>
plot_step <span class="op">=</span> <span class="fl">0.02</span>
class_names <span class="op">=</span> <span class="st">&quot;AB&quot;</span>

plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))

<span class="co"># This part finds out the prediction of the classifier on a grid that spans the entire space of the training data. This is how we shall find the </span>
<span class="co"># decision &quot;boundary&quot; of the classifier</span>
plt.subplot(<span class="dv">121</span>)
x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span>
y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span>
xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min, y_max, plot_step))

Z <span class="op">=</span> bdt.predict(np.c_[xx.ravel(), yy.ravel()])
Z <span class="op">=</span> Z.reshape(xx.shape)
cs <span class="op">=</span> plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Paired)
plt.axis(<span class="st">&quot;tight&quot;</span>)

<span class="co"># Plot the training points</span>
<span class="cf">for</span> i, n, c <span class="kw">in</span> <span class="bu">zip</span>(<span class="bu">range</span>(<span class="dv">2</span>), class_names, plot_colors):
    idx <span class="op">=</span> np.where(y <span class="op">==</span> i)
    plt.scatter(X[idx, <span class="dv">0</span>], X[idx, <span class="dv">1</span>],
                c<span class="op">=</span>c, cmap<span class="op">=</span>plt.cm.Paired,
                label<span class="op">=</span><span class="st">&quot;Class </span><span class="sc">%s</span><span class="st">&quot;</span> <span class="op">%</span> n)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)
plt.xlabel(<span class="st">'x'</span>)
plt.ylabel(<span class="st">'y'</span>)
plt.title(<span class="st">'Decision Boundary'</span>)

plt.tight_layout()
plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.35</span>)
plt.show()</code></pre></div>
<div class="figure">
<img src="../images/code8_2.png" alt="Generated Image" />
<p class="caption">Generated Image</p>
</div>


        </div>

        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
